{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "src/\n",
    "├── rag/\n",
    "│   ├── __init__.py                    # Initialize the RAG module\n",
    "│   ├── text_files/\n",
    "│   │   ├── __init__.py                # Initialize text files processing\n",
    "│   │   ├── pdf_processor.py           # Handles RAG for PDF files\n",
    "│   │   ├── docx_processor.py          # Handles RAG for DOCX files\n",
    "│   │   ├── txt_processor.py           # Handles RAG for TXT files\n",
    "│   │   └── utils.py                   # Common utilities for text file processing\n",
    "│   ├── web_links/\n",
    "│   │   ├── __init__.py                # Initialize web links processing\n",
    "│   │   ├── web_scraper.py             # Scrapes and preprocesses data from web links\n",
    "│   │   └── utils.py                   # Common utilities for web links processing\n",
    "│   ├── github_repo/\n",
    "│   │   ├── __init__.py                # Initialize GitHub repository processing\n",
    "│   │   ├── repo_scraper.py            # Extracts and preprocesses data from GitHub repositories\n",
    "│   │   └── utils.py                   # Common utilities for GitHub repo processing\n",
    "│   └── rag_pipeline.py                # Combines RAG components into a cohesive pipeline\n",
    "├── llm/\n",
    "│   ├── __init__.py                    # Initialize the LLM module\n",
    "│   ├── openai_integration.py          # Integration with OpenAI models\n",
    "│   ├── groq_integration.py            # Integration with Groq models\n",
    "│   ├── llama_integration.py           # Integration with Llama models\n",
    "│   ├── huggingface_integration.py     # Integration with HuggingFace models\n",
    "│   ├── api_clients/\n",
    "│   │   ├── __init__.py                # Initialize API clients\n",
    "│   │   ├── openai_client.py           # API client for OpenAI\n",
    "│   │   ├── groq_client.py             # API client for Groq models\n",
    "│   │   ├── llama_client.py            # API client for Llama models\n",
    "│   │   ├── huggingface_client.py      # API client for HuggingFace\n",
    "│   └── llm_pipeline.py                # Combines LLM integrations into a cohesive pipeline\n",
    "├── utils/\n",
    "│   ├── __init__.py                    # Initialize the utilities module\n",
    "│   ├── file_utils.py                  # Generic utilities for file handling\n",
    "│   ├── web_utils.py                   # Generic utilities for web-related tasks\n",
    "│   └── data_preprocessing.py          # Common preprocessing tasks for both RAG and LLM\n",
    "├── config/\n",
    "│   ├── __init__.py                    # Initialize the configuration module\n",
    "│   ├── settings.py                    # Central configuration settings\n",
    "│   ├── secrets.py                     # Store API keys and sensitive information (use environment variables)\n",
    "└── main.py                            # Main entry point to initialize and run the application\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a playground!\n",
    "\n",
    "Here you can quickly test some code, visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.chdir('..')\n",
    "os.getcwd()\n",
    "\n",
    "from src.rag.web_scraper import WebScraper\n",
    "from src.rag.document_loader import URLDocumentLoader, FileDocumentLoader, GitHubIssuesDocumentLoader, GitHubRepoDocumentLoader, ResearchPapersDocumentLoader\n",
    "from src.rag.text_splitter import TextSplitter\n",
    "from src.rag.vector_store import VectorStoreManager\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "# GitHub API keys\n",
    "github_username = os.getenv('GITHUB_USERNAME')\n",
    "github_personal_token = os.getenv('GITHUB_PERSONAL_TOKEN')\n",
    "\n",
    "# AI models API keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "huggingface_api_key_2 = os.getenv('HUGGINGFACE_API_KEY_2')\n",
    "\n",
    "# Vector Database API keys\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_index_name = os.getenv('PINECONE_INDEX_NAME')\n",
    "\n",
    "# Research Papers API keys\n",
    "elsevier_api_key = os.getenv('ELSEVIER_API_KEY')\n",
    "ieee_api_key =  os.getenv('IEEE_API_KEY')\n",
    "elsevier_api_secret = os.getenv('ELSEVIER_API_SECRET')\n",
    "\n",
    "# MongoDB API keys\n",
    "mongo_username = os.getenv('MONGO_USERNAME')\n",
    "mongo_password = os.getenv('MONGO_PASSWORD')\n",
    "mongo_cluster = os.getenv('MONGO_CLUSTER')\n",
    "mongo_db = os.getenv('MONGO_DB')\n",
    "mongo_collections = os.getenv('MONGO_COLLECTIONS')\n",
    "mongo_app_name = os.getenv('MONGO_APP_NAME')\n",
    "\n",
    "\n",
    "scraper = WebScraper()\n",
    "base_url = \"https://ou.edu/\"\n",
    "repo_dir = \"n8n-io/n8n\" # GitHub repository to scrape\n",
    "repo_url = \"https://github.com/obinopaul/licence-plate-detection.git\"   # URL of the GitHub repository to scrape\n",
    "clone_dir = \"./data\"  # Directory to clone the repository to\n",
    "\n",
    "query = \"What is the best way to detect license plates in images?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.web_scraper import WebScraper # Import the WebScraper class from the RAG module \n",
    "\n",
    "scraper = WebScraper()\n",
    "base_url = \"https://ou.edu/\"\n",
    "repo_dir = \"n8n-io/n8n\" # GitHub repository to scrape\n",
    "repo_url = \"https://github.com/obinopaul/licence-plate-detection.git\"   # URL of the GitHub repository to scrape    \n",
    "clone_dir = \"./data\"  # Directory to clone the repository to\n",
    "\n",
    "# Webscraper API call\n",
    "result = scraper.api_get_weblinks(base_url=base_url, time_limit=5)\n",
    "\n",
    "#--------------- document_loader.py ----------------\n",
    "\n",
    "datasets = URLDocumentLoader(base_url).load_from_urls() # Load data from URLs, takes in a list of URLs from webscraper.py\n",
    "datasets = FileDocumentLoader().load_from_files([os.path.join(os.getcwd(), 'data', file) for file in os.listdir('data')]) # Load data from text files\n",
    "datasets = GitHubIssuesDocumentLoader(access_token=github_personal_token).load_documents(repos=repo_dir) # Load data from GitHub issues \n",
    "datasets = GitHubRepoDocumentLoader(repo_urls=repo_url, clone_dir=clone_dir).load_documents_as_dicts()\n",
    "datasets = ResearchPapersDocumentLoader(\n",
    "    ieee_api_key = ieee_api_key,\n",
    "    elsevier_api_key = elsevier_api_key,\n",
    ").load_documents(query='machine learning', max_results=15, sources=['ieee', 'elsevier', 'arxiv'])\n",
    "\n",
    "\n",
    "#--------------- text_splitter.py ----------------\n",
    "\n",
    "splitter = TextSplitter(splitter_type = 'recursive', chunk_size = 1000, chunk_overlap = 200)\n",
    "# split files into chunks\n",
    "datas_chunks = splitter.split_file_documents([os.path.join(os.getcwd(), 'data', file) for file in os.listdir('data')])\n",
    "\n",
    "# Split url content into chunks\n",
    "datas_chunks = splitter.split_url_documents(base_url)\n",
    "\n",
    "# split github issues into chunks\n",
    "datas_chunks = splitter.split_github_issues_documents(repos=repo_dir, access_token=github_personal_token)\n",
    "\n",
    "# split github repo content into chunks\n",
    "datas_chunks = splitter.split_github_repo_documents(repo_urls=repo_url, clone_dir=clone_dir)\n",
    "\n",
    "# split research papers into chunks\n",
    "datas_chunks = splitter.split_research_papers_documents(query=query, max_results = 15, \n",
    "                                                             sources = ['ieee', 'elsevier', 'arxiv'], ieee_api_key = ieee_api_key,\n",
    "                                                             elsevier_api_key = elsevier_api_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------- vector_store.py ----------------\n",
    "\n",
    "from src.rag.vector_store import VectorStoreManager\n",
    "from src.rag.text_splitter import TextSplitter\n",
    "import os\n",
    "\n",
    "splitter = TextSplitter(splitter_type = 'recursive', chunk_size = 1000, chunk_overlap = 200)\n",
    "# split files into chunks\n",
    "datas_chunks = splitter.split_file_documents([os.path.join(os.getcwd(), 'data', file) for file in os.listdir('data')])\n",
    "\n",
    "\n",
    "# Initialize the VectorStoreManager\n",
    "vector_store_manager = VectorStoreManager(\n",
    "    vector_store_type=\"pinecone\",\n",
    "    embedding_type=\"openai\",\n",
    "    pinecone_api_key=pinecone_api_key,\n",
    "    pinecone_index_name=pinecone_index_name\n",
    ")\n",
    "\n",
    "# Add file chunks to the vector store\n",
    "vector_store_manager.add_documents(documents=datas_chunks)\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "search_results = vector_store_manager.search(query=query, top_k=5)\n",
    "\n",
    "print(\"Similarity Search Results:\")\n",
    "for result in search_results:\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(f\"Document: {result['document']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "# Create a retriever with default settings\n",
    "retriever = vector_store_manager.as_retriever()\n",
    "\n",
    "# Create a retriever with custom settings\n",
    "retriever_with_threshold = vector_store_manager.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        'k': 5,  # Retrieve 5 documents\n",
    "        'score_threshold': 0.7  # Only return documents with score >= 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "#--------------- other functions of vector store ---------------------------------------\n",
    "# Update a document\n",
    "document_id = \"document_id_to_update\"\n",
    "updated_content = \"Updated document content\"\n",
    "updated_document = Document(page_content=updated_content)\n",
    "vector_store_manager.update_document(document_id=document_id, updated_document=updated_document)\n",
    "\n",
    "\n",
    "# Retrieve a document by ID\n",
    "retrieved_document = vector_store_manager.get_document_by_id(document_id=document_id)\n",
    "print(f\"Retrieved Document: {retrieved_document.page_content}\")\n",
    "\n",
    "# Delete a document\n",
    "vector_store_manager.delete_document(document_id=document_id)\n",
    "\n",
    "# Clear the entire vector store\n",
    "vector_store_manager.clear_vector_store()\n",
    "\n",
    "# Get vector store information\n",
    "vector_store_info = vector_store_manager.get_vector_store_info()\n",
    "print(f\"Vector Store Information: {vector_store_info}\")\n",
    "\n",
    "# List all indexes\n",
    "indexes = vector_store_manager.list_indexes()\n",
    "print(f\"Indexes: {indexes}\")\n",
    "\n",
    "# Delete the index\n",
    "vector_store_manager.delete_index()\n",
    "\n",
    "\n",
    "\n",
    "#--------------- data_validate.py ----------------\n",
    "from src.data_validate import DataValidator\n",
    "from pydantic import ValidationError\n",
    "\n",
    "data_types_examples = [\"url\", \"text\", \"github_repo_url\", \"github_repo_text\"]\n",
    "validator = DataValidator()\n",
    "\n",
    "# EXAMPLE 1: Valid URL\n",
    "try:\n",
    "    validated_1 = validator.validate_input(data_type=\"github_repo_text\", data=\"example.com/docs\")\n",
    "    print(\"Validated 1:\", validated_1)\n",
    "except ValidationError as e:\n",
    "    print(\"Validation Error 1:\", e)\n",
    "\n",
    "\n",
    "#--------------- llm.py ----------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------- llm_pipeline.py ----------------\n",
    "from src.llm.llm_pipeline import LLMPipeline\n",
    "\n",
    "openai_pipeline = LLMPipeline(model_type=\"openai\", api_key=openai_api_key, temperature=0.7)\n",
    "openai_pipeline.chatbot.create_retrieval_qa_chain(retriever=retriever)\n",
    "\n",
    "openai_response = openai_pipeline.run(\"What's the capital of France?\")\n",
    "print(\"OpenAI Response:\")\n",
    "print(openai_response)\n",
    "\n",
    "query = \"What are the main advantages of using Python for data science?\"\n",
    "rag_response = openai_pipeline.generate_response(query, retriever)\n",
    "print(\"Retrieval-Augmented Generation Response:\")\n",
    "print(rag_response)\n",
    "\n",
    "rag_response_with_sources = openai_pipeline.generate_response_with_sources(query, retriever)\n",
    "print(\"Retrieval-Augmented Generation Response with Sources:\")\n",
    "print(rag_response_with_sources[\"result\"])\n",
    "print(\"Sources:\")\n",
    "for source in rag_response_with_sources[\"sources\"]:\n",
    "    print(source.metadata[\"source\"])\n",
    "\n",
    "\n",
    "\n",
    "#--------------- run_rag_pipeline.py ----------------\n",
    "from src.run_rag_pipeline import RunChatbot\n",
    "\n",
    "data_task = [\"file\", \"url\", \"research_papers\", \"github_repo\", \"github_issues\"] #github_issues needs the repo_text while github_repo needs the repo_url\n",
    "\n",
    "chatbot = RunChatbot(\n",
    "    model_type=\"openai\",\n",
    "    api_key=openai_api_key,\n",
    "    use_rag=False,\n",
    "    # data_task=\"file\",  # or \"url\", \"github_repo\", etc.\n",
    "    # data_value=\"./data\",  # path to the folder  \"./data for file\"\n",
    "    vector_store_type=\"pinecone\",\n",
    "    embedding_type=\"openai\",\n",
    "    temperature=0.7,\n",
    "    # the below are optional\n",
    "    github_access_token=github_personal_token,\n",
    "    ieee_api_key=ieee_api_key,\n",
    "    pinecone_api_key=pinecone_api_key,\n",
    ")\n",
    "\n",
    "# 1) Validate & Split\n",
    "chatbot.setup_data()\n",
    "# 2) Build VectorStore, add docs, get retriever\n",
    "chatbot.setup_vector_store()\n",
    "# 3) Create LLM pipeline, build chain\n",
    "chatbot.setup_llm_pipeline()\n",
    "# 4) Chat\n",
    "query = \"How can i perform rag using langchain_ollama and chromadb\"\n",
    "result = chatbot.chat(query, with_sources=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Chatbot with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.rag.text_splitter:Loading file documents from paths: ['c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\1206.4612v1.pdf', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Cost-Sensitive_Online_Classification.pdf', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Cytovance data.docx', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Innovation District Student Interview Questions.docx', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\licence-plate-detection', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Manuscript_template.docx', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Online Passive-Aggressive Active learning.pdf', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\wang2012.pdf']\n",
      "WARNING:src.rag.document_loader:Unsupported file type for file c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\data\\licence-plate-detection. Skipping.\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 194 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 195 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 207 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 216 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 218 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 219 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 220 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 221 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 230 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 231 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 232 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 233 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 234 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 252 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 253 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 254 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 255 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 267 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 284 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 286 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 287 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 288 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 289 0 (offset 0)\n",
      "INFO:src.rag.text_splitter:Splitting 74 documents using recursive splitter.\n",
      "INFO:src.rag.text_splitter:Split into 319 document chunks.\n",
      "INFO:src.rag.vector_store:Initializing embeddings with type: openai\n",
      "c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\src\\rag\\vector_store.py:94: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.embeddings = OpenAIEmbeddings(\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:src.rag.vector_store:Embeddings initialized successfully.\n",
      "INFO:src.rag.vector_store:Initializing vector store with type: pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\pault\\\\anaconda3\\\\envs\\\\app_project\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:src.rag.vector_store:Pinecone vector store initialized with index 'obinopaul'.\n",
      "INFO:src.rag.vector_store:Vector store initialized successfully.\n",
      "INFO:src.rag.vector_store:Adding 319 documents to the vector store.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:src.rag.vector_store:Documents added to the vector store successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.rag.vector_store import VectorStoreManager\n",
    "from src.rag.text_splitter import TextSplitter\n",
    "from src.llm.llm_pipeline import LLMPipeline\n",
    "import os\n",
    "\n",
    "base_url = \"https://ou.edu/\"\n",
    "repo_dir = \"n8n-io/n8n\" # GitHub repository to scrape\n",
    "repo_url = \"https://github.com/obinopaul/licence-plate-detection.git\"   # URL of the GitHub repository to scrape    \n",
    "clone_dir = \"./data\"  # Directory to clone the repository to\n",
    "\n",
    "splitter = TextSplitter(splitter_type = 'recursive', chunk_size = 1000, chunk_overlap = 200)\n",
    "\n",
    "#------------------ Validate data ------------------\n",
    "\n",
    "from src.data_validate import DataValidator\n",
    "from pydantic import ValidationError\n",
    "\n",
    "data_types_examples = [\"url\", \"text\", \"github_repo_url\", \"github_repo_text\"]\n",
    "validator = DataValidator()\n",
    "\n",
    "# EXAMPLE 1: Valid URL\n",
    "try:\n",
    "    validated_data = validator.validate_input(data_type=\"github_repo_url\", data=\"https://github.com/docs\")\n",
    "    print(\"Validated 1:\", validated_1)\n",
    "except ValidationError as e:\n",
    "    print(\"Validation Error 1:\", e)\n",
    "    \n",
    "    \n",
    "#------------------ Continue code ------------------   \n",
    "# split files into chunks\n",
    "datas_chunks = splitter.split_file_documents([os.path.join(os.getcwd(), 'data', file) for file in os.listdir('data')])\n",
    "# datas_chunks = splitter.split_url_documents(base_url)   # split url content into chunks\n",
    "# datas_chunks = splitter.split_github_issues_documents(repos=repo_dir, access_token=github_personal_token)   # split github issues into chunks\n",
    "# datas_chunks = splitter.split_github_repo_documents(repo_urls=repo_url, clone_dir=clone_dir)   # split github repo content into chunks\n",
    "# datas_chunks = splitter.split_research_papers_documents(query=query, max_results = 15, sources = ['ieee', 'elsevier', 'arxiv'], ieee_api_key = ieee\n",
    "\n",
    "\n",
    "# Initialize the VectorStoreManager\n",
    "vector_store_manager = VectorStoreManager(\n",
    "    vector_store_type=\"pinecone\",\n",
    "    embedding_type=\"openai\",\n",
    "    pinecone_api_key=pinecone_api_key,\n",
    "    pinecone_index_name=pinecone_index_name\n",
    ")\n",
    "\n",
    "# Add file chunks to the vector store\n",
    "vector_store_manager.add_documents(documents=datas_chunks)\n",
    "retriever = vector_store_manager.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "openai_pipeline = LLMPipeline(model_type=\"openai\", api_key=openai_api_key, temperature=0.7)\n",
    "openai_pipeline.chatbot.create_retrieval_qa_chain(retriever=retriever)\n",
    "\n",
    "# openai_response = openai_pipeline.run(\"What's the capital of France?\")\n",
    "# print(\"OpenAI Response:\")\n",
    "# print(openai_response)\n",
    "\n",
    "rag_response_with_sources = openai_pipeline.generate_response_with_sources(query, retriever)\n",
    "print(\"Retrieval-Augmented Generation Response with Sources:\")\n",
    "print(rag_response_with_sources[\"result\"])\n",
    "print(\"Sources:\")\n",
    "for source in rag_response_with_sources[\"sources\"]:\n",
    "    print(source.metadata[\"source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.run_rag_pipeline:=== [RunChatbot] setup_data ===\n",
      "WARNING:src.run_rag_pipeline:Validation error: 2 validation errors for DataPayload\n",
      "data_type\n",
      "  Input should be 'url', 'text', 'github_repo_url' or 'github_repo_text' [type=literal_error, input_value='file', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/literal_error\n",
      "data\n",
      "  Input should be a valid string [type=string_type, input_value=['C:\\\\Users\\\\pault\\\\Docum...be\\\\Paul_Okafor_CV.pdf'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/string_type\n",
      "INFO:src.rag.text_splitter:Loading file documents from paths: ['C:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\uploads\\\\7902e860-3c80-42a0-b467-a610268d61be\\\\Paul_Okafor_CV.pdf']\n",
      "INFO:src.rag.text_splitter:Splitting 1 documents using recursive splitter.\n",
      "INFO:src.rag.text_splitter:Split into 6 document chunks.\n",
      "INFO:src.run_rag_pipeline:Total splitted docs: 6\n",
      "INFO:src.run_rag_pipeline:=== [RunChatbot] setup_vector_store ===\n",
      "INFO:src.rag.vector_store:Initializing embeddings with type: openai\n",
      "c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\src\\rag\\vector_store.py:93: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.embeddings = OpenAIEmbeddings(\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:src.rag.vector_store:Embeddings initialized successfully.\n",
      "INFO:src.rag.vector_store:Initializing vector store with type: pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\pault\\\\anaconda3\\\\envs\\\\app_project\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:src.rag.vector_store:Pinecone vector store initialized with index 'langchain-index'.\n",
      "INFO:src.rag.vector_store:Vector store initialized successfully.\n",
      "INFO:src.rag.vector_store:Adding 6 documents to the vector store.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:src.rag.vector_store:Documents added to the vector store successfully.\n",
      "INFO:src.run_rag_pipeline:Documents added to VectorStoreManager.\n",
      "INFO:src.run_rag_pipeline:Retriever created successfully.\n",
      "INFO:src.run_rag_pipeline:=== [RunChatbot] setup_llm_pipeline ===\n",
      "c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\src\\llm\\openai.py:59: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(\n",
      "INFO:src.run_rag_pipeline:LLMPipeline created successfully.\n",
      "INFO:src.run_rag_pipeline:Retrieval QA chain established in the pipeline.\n"
     ]
    }
   ],
   "source": [
    "from src.run_rag_pipeline import RunChatbot\n",
    "\n",
    "data_task = [\"file\", \"url\", \"research_papers\", \"github_repo\", \"github_issues\"] #github_issues needs the repo_text while github_repo needs the repo_url\n",
    "\n",
    "chatbot = RunChatbot(\n",
    "    model_type=\"openai\",    # or \"openai\", \"llama\", \"huggingface\"\n",
    "    api_key=openai_api_key,\n",
    "    use_rag=True,\n",
    "    data_task=\"file\",  # or \"url\", \"github_repo\", etc.\n",
    "    data_value=['C:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\uploads\\\\7902e860-3c80-42a0-b467-a610268d61be\\\\Paul_Okafor_CV.pdf'],  # path to the folder  \"./data for file\"\n",
    "    # data_value= [r\"C:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\uploads\\d260beae-c00e-423a-950b-64b82f99fa83\\Paul_Okafor_CV.pdf\"],\n",
    "    vector_store_type=\"pinecone\",\n",
    "    embedding_type=\"openai\",\n",
    "    temperature=0.7,\n",
    "    # the below are optional\n",
    "    github_access_token=github_personal_token,\n",
    "    ieee_api_key=ieee_api_key,\n",
    "    pinecone_api_key=pinecone_api_key,\n",
    "    model = \"gpt-4-turbo\"\n",
    ")\n",
    "\n",
    "# 1) Validate & Split\n",
    "chatbot.setup_data()\n",
    "# 2) Build VectorStore, add docs, get retriever\n",
    "chatbot.setup_vector_store()\n",
    "# 3) Create LLM pipeline, build chain\n",
    "chatbot.setup_llm_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.run_rag_pipeline:=== [RunChatbot] chat - query: Tell me a little about the person in the attached document ===\n",
      "c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\src\\llm\\openai.py:252: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.conversation_chain({\"question\": query})\n",
      "INFO:src.rag.vector_store:Performing similarity search for query: 'Tell me a little about the person in the attached document' with top_k: 4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:src.rag.vector_store:Retrieved 4 similar documents.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but as a text-based AI, I don't have the ability to access or analyze any attached documents or images. However, if you provide me with some text-based information, I would be more than happy to help answer your question or provide further information.\n"
     ]
    }
   ],
   "source": [
    "# 4) Chat\n",
    "query = \"Tell me a little about the person in the attached document\"\n",
    "result = chatbot.chat(query, with_sources=True)\n",
    "\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the YAML configuration file\n",
    "# with open(\"config.yaml\", \"r\") as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "\n",
    "# # Optionally, overwrite secrets from environment variables for better security\n",
    "# config[\"secrets\"][\"openai_api_key\"] = os.getenv(\"OPENAI_API_KEY\", config[\"secrets\"][\"openai_api_key\"])\n",
    "# config[\"secrets\"][\"github_access_token\"] = os.getenv(\"GITHUB_TOKEN\", config[\"secrets\"][\"github_access_token\"])\n",
    "# # Add more overwrites as needed for other secrets\n",
    "\n",
    "# # Initialize the RunChatbot with config parameters\n",
    "# chatbot = RunChatbot(\n",
    "#     model_type=config[\"model_settings\"][\"model_type\"],\n",
    "#     api_key=config[\"secrets\"][\"openai_api_key\"],  # or other keys based on model_type\n",
    "#     use_rag=config[\"rag_settings\"][\"use_rag\"],\n",
    "#     data_task=config[\"data_task\"][\"type\"],\n",
    "#     data_value=config[\"data_task\"][\"value\"],\n",
    "#     vector_store_type=config[\"vector_store\"][\"type\"],\n",
    "#     embedding_type=config[\"embedding\"][\"type\"],\n",
    "#     model_name=config[\"embedding\"].get(\"model_name\"),  # or model_path for llama\n",
    "#     temperature=config[\"model_settings\"][config[\"model_settings\"][\"model_type\"]][\"temperature\"],\n",
    "#     max_tokens=config[\"model_settings\"][config[\"model_settings\"][\"model_type\"]][\"max_tokens\"],\n",
    "#     top_p=config[\"model_settings\"][config[\"model_settings\"][\"model_type\"]][\"top_p\"],\n",
    "#     freq_penalty=config[\"model_settings\"][config[\"model_settings\"][\"model_type\"]][\"frequency_penalty\"],\n",
    "#     pres_penalty=config[\"model_settings\"][config[\"model_settings\"][\"model_type\"]][\"presence_penalty\"],\n",
    "#     chunk_size=config[\"text_splitter\"][\"chunk_size\"],\n",
    "#     chunk_overlap=config[\"text_splitter\"][\"chunk_overlap\"],\n",
    "#     # Pass any additional kwargs from advanced_settings or others\n",
    "#     search_type=config[\"advanced_settings\"][\"search_type\"],\n",
    "#     score_threshold=config[\"advanced_settings\"].get(\"score_threshold\", 0.5),\n",
    "#     prompt_template=config[\"advanced_settings\"].get(\"prompt_template\"),\n",
    "#     enable_streaming=config[\"advanced_settings\"].get(\"enable_streaming\", False),\n",
    "#     # Add other parameters as needed\n",
    "#     clone_dir=config[\"data_task\"].get(\"github_repo\", {}).get(\"clone_dir\", \"./cloned_repo\"),\n",
    "#     github_access_token=config[\"data_task\"].get(\"github_issues\", {}).get(\"access_token\", \"\"),\n",
    "#     max_results=config[\"data_task\"].get(\"research_papers\", {}).get(\"max_results\", 15),\n",
    "#     sources=config[\"data_task\"].get(\"research_papers\", {}).get(\"sources\", [\"ieee\", \"elsevier\", \"arxiv\"]),\n",
    "#     # Any other nested configs\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete code to run it without our API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.rag.document_loader:Unsupported file type for file c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\data\\licence-plate-detection. Skipping.\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 194 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 195 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 207 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 216 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 218 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 219 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 220 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 221 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 230 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 231 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 232 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 233 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 234 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 252 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 253 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 254 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 255 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 267 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 284 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 286 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 287 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 288 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 289 0 (offset 0)\n",
      "INFO:src.rag.text_splitter:Loading file documents from paths: ['c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\1206.4612v1.pdf', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Cost-Sensitive_Online_Classification.pdf', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Cytovance data.docx', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Innovation District Student Interview Questions.docx', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\licence-plate-detection', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Manuscript_template.docx', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\Online Passive-Aggressive Active learning.pdf', 'c:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\LLM App\\\\data\\\\wang2012.pdf']\n",
      "WARNING:src.rag.document_loader:Unsupported file type for file c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\LLM App\\data\\licence-plate-detection. Skipping.\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 194 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 195 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 207 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 216 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 218 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 219 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 220 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 221 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 230 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 231 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 232 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 233 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 234 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 252 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 253 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 254 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 255 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 267 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 284 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 286 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 287 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 288 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 289 0 (offset 0)\n",
      "INFO:src.rag.text_splitter:Splitting 74 documents using recursive splitter.\n",
      "INFO:src.rag.text_splitter:Split into 319 document chunks.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\pault\\\\anaconda3\\\\envs\\\\app_project\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAI as LangchainOpenAI\n",
    "from langchain_openai import ChatOpenAI as LangchainChatOpenAI \n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.vectorstores import Pinecone\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from uuid import uuid4\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "from src.rag.document_loader import URLDocumentLoader, FileDocumentLoader, GitHubIssuesDocumentLoader, GitHubRepoDocumentLoader, ResearchPapersDocumentLoader\n",
    "from src.rag.text_splitter import TextSplitter\n",
    "from src.rag.document_loader import FileDocumentLoader\n",
    "\n",
    "\n",
    "datasets = FileDocumentLoader().load_from_files([os.path.join(os.getcwd(), 'data', file) for file in os.listdir('data')]) # Load data from text files\n",
    "splitter = TextSplitter(splitter_type = 'recursive', chunk_size = 1000, chunk_overlap = 200)\n",
    "# split files into chunks\n",
    "datas_chunks = splitter.split_file_documents([os.path.join(os.getcwd(), 'data', file) for file in os.listdir('data')])\n",
    "\n",
    "#---------------------Vector Store---------------------\n",
    "embeddings = OpenAIEmbeddings(\n",
    "                model=\"text-embedding-ada-002\",  # You can make this configurable\n",
    "                # disallowed_search=(),\n",
    "                openai_api_key=openai_api_key\n",
    "            )\n",
    "dummy_vector = embeddings.embed_query(\"test dimension\")\n",
    "dimension = len(dummy_vector)\n",
    "\n",
    "pc = Pinecone(pinecone_api_key=pinecone_api_key)\n",
    "pinecone_index_name = \"langchain-index\"\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "if pinecone_index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "            name=pinecone_index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "\n",
    "index = pc.Index(pinecone_index_name)\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "ids = [str(uuid4()) for _ in range(len(datas_chunks))]\n",
    "vector_store.add_documents(documents=datas_chunks, ids=ids)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "#---------------------RAG Pipeline---------------------\n",
    "llm = LangchainChatOpenAI(api_key=openai_api_key, model=\"gpt-4\")\n",
    "\n",
    "\n",
    "\n",
    "# System instructions\n",
    "system_template = \"\"\"\n",
    "You are a helpful assistant with access to the following context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Follow these rules:\n",
    "1. If the answer is not in the context, say \"I don't know\".\n",
    "2. Keep your answer brief, under three sentences.\n",
    "\"\"\"\n",
    "\n",
    "# We can build a chat prompt that includes:\n",
    "#  - System message template\n",
    "#  - A placeholder for conversation history\n",
    "#  - A user question\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    # MessagesPlaceholder(variable_name=\"history\"),  # Will hold conversation memory\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Pass the `chat_prompt` as the \"combine_docs_chain\" argument\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,              # Optionally see which docs were used\n",
    "    combine_docs_chain_kwargs={\"prompt\": chat_prompt}\n",
    ")\n",
    "\n",
    "# chat_history = []\n",
    "\n",
    "user_query = input(\"User: \")\n",
    "if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "    print(\"Exiting the chat.\")\n",
    "\n",
    "# The chain expects a dict with {\"question\": <user query>} \n",
    "result = conversation_chain({\"question\": user_query})\n",
    "\n",
    "# \"answer\" is the final text from the LLM\n",
    "print(\"Assistant:\", result[\"answer\"])\n",
    "print(result[\"source_documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
